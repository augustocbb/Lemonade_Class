#!/usr/bin/python
# -*- coding: utf-8 -*-
"""
Auto-generated PySpark code from Lemonade Workflow.
(c) Speed Labs - Departamento de Ciência da Computação
    Universidade Federal de Minas Gerais

 This code was generated by Lemonade(http://www.lemonade.org.br), 
 a tool for design data processing and machine learning workflows.
 Targeting platform is Apache Spark. In order to be able to run this code, you must:
 1. Install all dependencies for Lemonade;
 2. Configure connection parameters for Apache Spark and HDFS;
 3. Review the code and make any necessary adjust.
    
"""

# Not all imports are really necessary, remove them accordingly.\n",
from concurrent.futures import ThreadPoolExecutor
import collections
import datetime
import itertools
import json
import os
import re
import simplejson
import string
import sys
import time
import threading
import traceback
import unicodedata
import juicer.spark.ext as juicer_ext

from gettext import gettext as _
from textwrap import dedent
from timeit import default_timer as timer

from pyspark.ml import classification, evaluation, feature, tuning, clustering
from pyspark.ml.linalg import Vectors
from pyspark.sql import functions, types, Row, DataFrame
from pyspark.sql.utils import IllegalArgumentException
from pyspark.sql.window import Window
from pyspark.ml.linalg import Vectors, VectorUDT

from pyspark.ml import Pipeline
from pyspark.ml.classification import *
from pyspark.ml.clustering import *
from pyspark.ml.evaluation import *
from pyspark.ml.feature import *
from pyspark.ml.tuning import *
from pyspark.ml.recommendation import *
from pyspark.ml.regression import *
from pyspark.mllib.evaluation import *
from juicer import privaaas
from juicer.util import dataframe_util 
from juicer.spark.reports import *
from juicer.spark.ml_operation import ModelsEvaluationResultList

def emit(df, n=20):
    """
    Print first n records from DataFrame
    """
    print(df.head(n))


def emit_schema(df):
    """
    Print a concise summary of a DataFrame.
    """
    df.info(verbose=True)


def main():
    """
    Generated code.
    """

    # ---------------------------------------------------------------------------
    # Read data
    # task_id = '7f45d4f3-8bc2-4941-b956-d4c05222da18'
    # ---------------------------------------------------------------------------
    schema_df_4 = types.StructType()
    schema_df_4.add('ano', types.IntegerType(), False)
    schema_df_4.add('codigo_ies', types.IntegerType(), False)
    schema_df_4.add('sigla_ies', types.StringType(), False)
    schema_df_4.add('nome_campus', types.StringType(), False)
    schema_df_4.add('uf_campus', types.StringType(), False)
    schema_df_4.add('municipio_campus', types.StringType(), False)
    schema_df_4.add('nome_curso', types.StringType(), False)
    schema_df_4.add('grau', types.StringType(), False)
    schema_df_4.add('turno', types.StringType(), False)
    schema_df_4.add('mod_concorrencia', types.StringType(), False)
    schema_df_4.add('qt_vagas_concorrencia', types.IntegerType(), False)
    schema_df_4.add('sexo', types.StringType(), False)
    schema_df_4.add('data_nascimento', types.TimestampType(), False)
    schema_df_4.add('idade', types.IntegerType(), False)
    schema_df_4.add('uf_candidato', types.StringType(), False)
    schema_df_4.add('municipio_candidato', types.StringType(), False)
    schema_df_4.add('opcao', types.IntegerType(), False)
    schema_df_4.add('nota_l', types.DecimalType(7, 1), False)
    schema_df_4.add('nota_ch', types.DecimalType(7, 1), False)
    schema_df_4.add('nota_cn', types.DecimalType(7, 1), False)
    schema_df_4.add('nota_m', types.DecimalType(7, 1), False)
    schema_df_4.add('nota_r', types.IntegerType(), False)
    schema_df_4.add('nota_candidato', types.DecimalType(8, 2), False)
    schema_df_4.add('nota_corte', types.DecimalType(8, 2), False)
    schema_df_4.add('classificacao', types.IntegerType(), False)
    schema_df_4.add('aprovado', types.StringType(), False)
    schema_df_4.add('matricula', types.StringType(), False)

    url = 'hdfs://150.164.203.230:9000/user/lemonade/limonero/data/d308a2ccff3d4785812c1561dcc426af_Banco de dados.csv'
    jvm = spark_session._jvm
    jvm.java.lang.System.setProperty("HADOOP_USER_NAME", "hadoop")

    df_4 = spark_session.read.option('nullValue', '').option(
        'treatEmptyValuesAsNulls', 'true').option(
        'wholeFile', True).option(
            'multiLine', True).option('escape',
                                      '"').option('timestampFormat', 'MM/dd/yyyy'
                                                  ).csv(
            url, schema=schema_df_4,
            quote=None,
            ignoreTrailingWhiteSpace=True,  # Handles
            encoding='UTF-8',
            header=True, sep=',',
            inferSchema=False,
            mode='DROPMALFORMED')
    df_4.cache()
    emit(df_4)
    emit_schema(df_4)

    # ---------------------------------------------------------------------------
    # Select attribute(s)
    # task_id = 'f08a64bf-7bc9-4950-b910-fb5d36262374'
    # ---------------------------------------------------------------------------
    df_11 = df_4.select(
        "uf_campus",
        "turno",
        "qt_vagas_concorrencia",
        "aprovado",
        "nome_curso")

    # ---------------------------------------------------------------------------
    # Group by function
    # task_id = 'c46773db-eae9-4c0e-9a12-35187e0b8696'
    # ---------------------------------------------------------------------------
    pivot_values = None
    pivot_attr = ''
    if pivot_attr:
        df_8 = df_11.groupBy(
            functions.col('nome_curso')).pivot(
                pivot_attr, pivot_values).agg(
                    functions.count('nome_curso').alias('COUNT_INSCRITOS'))
    else:
        df_8 = df_11.groupBy(
            functions.col('nome_curso')).agg(
                functions.count('nome_curso').alias('COUNT_INSCRITOS'))

    # ---------------------------------------------------------------------------
    # Filter by function
    # task_id = '8a345b26-bff9-422b-bd2e-25cf3e5eb960'
    # ---------------------------------------------------------------------------
    df_6 = df_11.filter(
        ((df_11['aprovado'] == 'Sim')))

    # ---------------------------------------------------------------------------
    # Group by function
    # task_id = 'd53b1c23-bf08-4907-9020-6bf235b10cdb'
    # ---------------------------------------------------------------------------
    pivot_values = None
    pivot_attr = ''
    if pivot_attr:
        df_9 = df_6.groupBy(
            functions.col('nome_curso')).pivot(
                pivot_attr, pivot_values).agg(
                    functions.count('nome_curso').alias('COUNT_APROVADOS'))
    else:
        df_9 = df_6.groupBy(
            functions.col('nome_curso')).agg(
                functions.count('nome_curso').alias('COUNT_APROVADOS'))

    # ---------------------------------------------------------------------------
    # Join
    # task_id = '6c652de8-d155-4884-9b02-6afc01063d48'
    # ---------------------------------------------------------------------------
    conditions = [
        functions.lower(
            functions.col('first.nome_curso')) == functions.lower(
            functions.col('second.nome_curso'))
    ]
    result_df = df_8.alias('first').join(
        df_9.alias('second'),
        on=conditions, how='inner')

    first_attrs = [
        df_8['nome_curso'].alias('nome_curso'),
        df_8['COUNT_INSCRITOS'].alias('COUNT_INSCRITOS')
    ]

    second_attrs = [
        df_9['COUNT_APROVADOS'].alias('COUNT_APROVADOS')
    ]

    selected_attrs = first_attrs + second_attrs
    df_3 = result_df.select(*selected_attrs)

    # ---------------------------------------------------------------------------
    # Transform by function
    # task_id = 'e573a5cf-afcc-4ca8-8986-b5c2d6782231'
    # ---------------------------------------------------------------------------
    from juicer.spark.ext import CustomExpressionTransformer
    expr_alias = [
        [(df_3['COUNT_APROVADOS'] / df_3['COUNT_INSCRITOS']), 'TAXA_APROVACAO']
    ]
    tmp_out = df_3
    for expr, alias in expr_alias:
        transformer = CustomExpressionTransformer(outputCol=alias,
                                                  expression=expr)
        tmp_out = transformer.transform(tmp_out)
    df_10 = tmp_out

    # ---------------------------------------------------------------------------
    # Table
    # task_id = '1e4320dc-f6d4-4ace-b2d5-da7e485e1fd2'
    # ---------------------------------------------------------------------------
    from juicer.spark.vis_operation import TableVisualizationModel
    from juicer.util.dataframe_util import SimpleJsonEncoder as enc

    params = {
        'column_names': [
            'nome_curso',
            'COUNT_APROVADOS',
            'COUNT_INSCRITOS',
            'TAXA_APROVACAO'],
        'execution_date': None,
        'display_sample': False,
        'display_schema': False,
        'job_id': 0,
        'operation_id': 35,
        'operation_slug': 'table-visualization',
        'order': 7,
        'task_id': '1e4320dc-f6d4-4ace-b2d5-da7e485e1fd2',
        'plain': True,
        'user': {
            'id': 973,
            'name': 'Augusto Bastos',
            'login': 'augusto.cbb@hotmail.com'},
        'workflow_id': 8345,
        'workflow_name': 'PROJETO_1_SISU_2021',
        'parents_slug': ['transformation'],
        'multiplicity': {
            'input data': 1}}
    visua0 = TableVisualizationModel(
        df_10, '1e4320dc-f6d4-4ace-b2d5-da7e485e1fd2', '35',
        'table-visualization', '',
        ["nome_curso", "COUNT_APROVADOS", "COUNT_INSCRITOS", "TAXA_APROVACAO"],
        '', [], [],
        params=params)

    # ---------------------------------------------------------------------------
    # Filter by function
    # task_id = '846f8cce-d9fe-42ea-8267-c6de6a1015f2'
    # ---------------------------------------------------------------------------
    df_5 = df_11.filter(
        ((df_11['qt_vagas_concorrencia'] > 0)))

    # ---------------------------------------------------------------------------
    # Handle missing values
    # task_id = '4f7ab0fd-f53f-4519-b346-1335c0c08cf6'
    # ---------------------------------------------------------------------------
    attributes_df_5 = ["turno"]
    if len(attributes_df_5) > 0:
        df_1 = df_5.na.drop(how='any', subset=attributes_df_5)
    else:
        df_1 = df_5

    # ---------------------------------------------------------------------------
    # Group by function
    # task_id = 'ad81daab-7768-480c-b503-86ce1652f54e'
    # ---------------------------------------------------------------------------
    pivot_values = ['Integral', 'Matutino', 'Vespertino']
    pivot_attr = 'turno'
    if pivot_attr:
        df_7 = df_1.groupBy(
            functions.col('uf_campus')).pivot(
                pivot_attr, pivot_values).agg(
                    functions.avg('qt_vagas_concorrencia'))
    else:
        df_7 = df_1.groupBy(
            functions.col('uf_campus')).agg(
                functions.avg('qt_vagas_concorrencia'))

    # ---------------------------------------------------------------------------
    # Bar chart
    # task_id = '69abbc16-801c-4b14-a086-19ffde0ded63'
    # ---------------------------------------------------------------------------
    from juicer.spark.vis_operation import BarChartModel
    from juicer.util.dataframe_util import SimpleJsonEncoder as enc

    params = {
        'x_axis_attribute': ['uf_campus'],
        'display_mode': 'vertical',
        'column_names': [
            'Integral',
            'Matutino',
            'Vespertino'],
        'title': 'aa',
        'x_title': 'aad',
        'y_title': 'aasfd',
        'execution_date': None,
        'display_sample': False,
        'display_schema': False,
        'job_id': 0,
        'operation_id': 69,
        'operation_slug': 'bar-chart',
        'order': 11,
        'task_id': '69abbc16-801c-4b14-a086-19ffde0ded63',
        'plain': True,
        'user': {
            'id': 973,
            'name': 'Augusto Bastos',
            'login': 'augusto.cbb@hotmail.com'},
        'workflow_id': 8345,
        'workflow_name': 'PROJETO_1_SISU_2021',
        'parents_slug': ['aggregation'],
        'multiplicity': {
            'input data': 3}}
    visua2 = BarChartModel(
        df_7, '69abbc16-801c-4b14-a086-19ffde0ded63', '69',
        'bar-chart', 'aa',
        ["Integral", "Matutino", "Vespertino"],
        '', [], [],
        params=params)

    # ---------------------------------------------------------------------------
    # Publish as dashboard
    # task_id = '6fd06dc8-687c-40ba-ac29-4b963c86c1b3'
    # ---------------------------------------------------------------------------
    from juicer.service import caipirinha_service
    from juicer.util.dataframe_util import SimpleJsonEncoder as enc
    visualizations = []

    visualizations.append({
        'job_id': '0',
        'task_id': visua0.task_id,
        'title': visua0.title,
        'type': {
            'id': visua0.type_id,
        },
        'data': simplejson.dumps(
            visua0.get_data(), cls=enc, ignore_nan=True),
        'model': visua0
    })

    visualizations.append({
        'job_id': '0',
        'task_id': visua2.task_id,
        'title': visua2.title,
        'type': {
            'id': visua2.type_id,
        },
        'data': simplejson.dumps(
            visua2.get_data(), cls=enc, ignore_nan=True),
        'model': visua2
    })

    # Basic information to connect to other services
    config = {
        'juicer': {
            'services': {
                'limonero': {
                    'url': 'http://limonero:23402',
                    'auth_token': '9740600'
                },
                'caipirinha': {
                    'url': 'http://caipirinha:23401',
                    'auth_token': '9740600',
                    'storage_id': 2
                },
            }
        }
    }

    caipirinha_service.new_dashboard(config, 'EXEMPLO_SISU_2021', {'id': 973, 'name': 'Augusto Bastos', 'login': 'augusto.cbb@hotmail.com'},
                                     8345, u'PROJETO_1_SISU_2021',
                                     0, '6fd06dc8-687c-40ba-ac29-4b963c86c1b3', visualizations, emit_event)

    # ---------------------------------------------------------------------------
    # Read data
    # task_id = '030a81a4-341f-4515-9975-d2f7ec16a077'
    # ---------------------------------------------------------------------------
    schema_out_task_13 = types.StructType()
    schema_out_task_13.add('ano', types.IntegerType(), False)
    schema_out_task_13.add('codigo_ies', types.IntegerType(), False)
    schema_out_task_13.add('sigla_ies', types.StringType(), False)
    schema_out_task_13.add('nome_campus', types.StringType(), False)
    schema_out_task_13.add('uf_campus', types.StringType(), False)
    schema_out_task_13.add('municipio_campus', types.StringType(), False)
    schema_out_task_13.add('nome_curso', types.StringType(), False)
    schema_out_task_13.add('grau', types.StringType(), False)
    schema_out_task_13.add('turno', types.StringType(), False)
    schema_out_task_13.add('mod_concorrencia', types.StringType(), False)
    schema_out_task_13.add('qt_vagas_concorrencia', types.IntegerType(), False)
    schema_out_task_13.add('sexo', types.StringType(), False)
    schema_out_task_13.add('data_nascimento', types.TimestampType(), False)
    schema_out_task_13.add('idade', types.IntegerType(), False)
    schema_out_task_13.add('uf_candidato', types.StringType(), False)
    schema_out_task_13.add('municipio_candidato', types.StringType(), False)
    schema_out_task_13.add('opcao', types.IntegerType(), False)
    schema_out_task_13.add('nota_l', types.DecimalType(7, 1), False)
    schema_out_task_13.add('nota_ch', types.DecimalType(7, 1), False)
    schema_out_task_13.add('nota_cn', types.DecimalType(7, 1), False)
    schema_out_task_13.add('nota_m', types.DecimalType(7, 1), False)
    schema_out_task_13.add('nota_r', types.IntegerType(), False)
    schema_out_task_13.add('nota_candidato', types.DecimalType(8, 2), False)
    schema_out_task_13.add('nota_corte', types.DecimalType(8, 2), False)
    schema_out_task_13.add('classificacao', types.IntegerType(), False)
    schema_out_task_13.add('aprovado', types.StringType(), False)
    schema_out_task_13.add('matricula', types.StringType(), False)

    url = 'hdfs://150.164.203.230:9000/user/lemonade/limonero/data/d308a2ccff3d4785812c1561dcc426af_Banco de dados.csv'
    jvm = spark_session._jvm
    jvm.java.lang.System.setProperty("HADOOP_USER_NAME", "hadoop")

    out_task_13 = spark_session.read.option('nullValue', '').option(
        'treatEmptyValuesAsNulls', 'true').option(
        'wholeFile', True).option(
            'multiLine', True).option('escape',
                                      '"').option('timestampFormat', 'MM/dd/yyyy'
                                                  ).csv(
            url, schema=schema_out_task_13,
            quote=None,
            ignoreTrailingWhiteSpace=True,  # Handles
            encoding='UTF-8',
            header=True, sep=',',
            inferSchema=False,
            mode='DROPMALFORMED')
    out_task_13.cache()
    emit(out_task_13)
    emit_schema(out_task_13)


if __name__ == '__main__':
    main()
